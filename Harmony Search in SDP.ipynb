{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {},
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "import pandas \n",
    "from sklearn.tree import DecisionTreeClassifier\n",
    "from sklearn.metrics import accuracy_score, roc_auc_score, matthews_corrcoef, confusion_matrix, classification_report, f1_score\n",
    "from imblearn.over_sampling import SMOTE\n",
    "from sklearn.model_selection import train_test_split\n",
    "from sklearn.preprocessing import StandardScaler\n",
    "from sklearn.preprocessing import MinMaxScaler\n",
    "from sklearn.model_selection import KFold\n",
    "from bisect import bisect_left\n",
    "from tqdm import notebook\n",
    "import math"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "metadata": {},
   "outputs": [],
   "source": [
    "def feature_selection(X_train,X_test,vector):\n",
    "    index=[]\n",
    "    col_index=0\n",
    "    assert len(vector)!=62\n",
    "    for i in vector:\n",
    "        if i==1:\n",
    "            index.append(col_index)\n",
    "        col_index+=1\n",
    "    if not index:\n",
    "        index.append(0)\n",
    "    X_train=X_train[:,index]\n",
    "    X_test=X_test[:,index]\n",
    "    return X_train, X_test"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "metadata": {},
   "outputs": [],
   "source": [
    "def G_measure(y_true,y_pred): #OK\n",
    "    arr = confusion_matrix(y_true,y_pred)\n",
    "    pd = arr[1][1]/(arr[1][1]+arr[1][0])\n",
    "    pf = arr[0][1]/(arr[0][0]+arr[0][1])\n",
    "    fallout = 1-pf\n",
    "    g_measure = 2*((pd*fallout)/(pd+fallout))\n",
    "    return g_measure"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "metadata": {},
   "outputs": [],
   "source": [
    "# def confusion_matrix(y_true,y_pred): #OK\n",
    "#     arr = confusion_matrix(y_true,y_pred)\n",
    "#     return arr"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "metadata": {},
   "outputs": [],
   "source": [
    "def LIR(X_true, y_true, y_pred):\n",
    "    TN=FP=FN=TP=0\n",
    "    for i in range(len(y_true)):\n",
    "        if y_true[i]==False and y_pred[i]==False:\n",
    "            TN+=X_true[i][25]\n",
    "        elif y_true[i]==False and y_pred[i]==True:\n",
    "            FP+=X_true[i][25]\n",
    "        elif y_true[i]==True and y_pred[i]==False:\n",
    "            FN+=X_true[i][25]\n",
    "        else :\n",
    "            TP+=X_true[i][25]\n",
    "    LI = (TP+FP)/(TP+TN+FP+FN)\n",
    "    PV = (TP)/(TP+FN)\n",
    "    LIR = (PV-LI)/PV\n",
    "    return LIR"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "metadata": {},
   "outputs": [],
   "source": [
    "def FIR(y_true,y_pred):\n",
    "    arr = confusion_matrix(y_true,y_pred)\n",
    "    pd = arr[1][1]/(arr[1][1]+arr[1][0])\n",
    "    fi = (arr[0][1]+arr[1][1])/(arr[0][0]+arr[0][1]+arr[1][1]+arr[1][0])\n",
    "    FIR = (pd-fi)/pd\n",
    "    return FIR"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.preprocessing import MinMaxScaler\n",
    "from sklearn.preprocessing import StandardScaler\n",
    "def Norm(X_train,X_test,norm):\n",
    "    if norm==0: # None\n",
    "        return X_train,X_test\n",
    "    elif norm==1:#MinMax (all)\n",
    "        scaler=MinMaxScaler()\n",
    "        scaler.fit(np.concatenate((X_train, X_test), axis=0))\n",
    "        X_train_norm=scaler.transform(X_train)\n",
    "        X_test_norm=scaler.transform(X_test)\n",
    "        return X_train_norm, X_test_norm\n",
    "    elif norm==2:#z-score (all)\n",
    "        scaler=StandardScaler()\n",
    "        scaler.fit(np.concatenate((X_train, X_test), axis=0))\n",
    "        X_train_norm=scaler.transform(X_train)\n",
    "        X_test_norm=scaler.transform(X_test)\n",
    "        return X_train_norm, X_test_norm\n",
    "    elif norm==3:#z-score (source distribution)\n",
    "        scaler=StandardScaler()\n",
    "        scaler.fit(X_train)\n",
    "        X_train_norm=scaler.transform(X_train)\n",
    "        X_test_norm=scaler.transform(X_test)\n",
    "        return X_train_norm, X_test_norm\n",
    "    elif norm==4: #z-score (target distribution)\n",
    "        scaler=StandardScaler()\n",
    "        scaler.fit(X_test)\n",
    "        X_train_norm=scaler.transform(X_train)\n",
    "        X_test_norm=scaler.transform(X_test)\n",
    "        return X_train_norm, X_test_norm\n",
    "    elif norm==5: #logarithmic filtering\n",
    "        X_train_norm=np.log10(np.where(X_train<0.001,0.001,X_train))\n",
    "        X_test_norm=np.log10(np.where(X_test<0.001,0.001,X_test))\n",
    "        return X_train_norm, X_test_norm"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "metadata": {},
   "outputs": [],
   "source": [
    "def classifier(X_train,X_test,y_train,y_test,vector):\n",
    "    #Vector 0 : Criterion\n",
    "    print(vector[:(len(vector)-X_train.shape[1])])\n",
    "    X_true=X_test\n",
    "    if vector[0]==0:\n",
    "        criterion = \"gini\"\n",
    "    else:\n",
    "        criterion = \"entropy\"\n",
    "\n",
    "    #Vector 0 : Criterion\n",
    "    if vector[1]==0:\n",
    "        split = \"best\"\n",
    "    else:\n",
    "        split = \"random\"\n",
    "\n",
    "    #Vectro 6: Class Weight\n",
    "    balance = balance = {True:1,False:vector[8]}\n",
    "\n",
    "    #Model Construct\n",
    "    clf = DecisionTreeClassifier(criterion=criterion,\n",
    "                                 splitter=split,\n",
    "                                 max_depth=vector[2],\n",
    "                                 min_samples_split=vector[3],\n",
    "                                 min_samples_leaf=vector[4],\n",
    "                                 min_weight_fraction_leaf=vector[5],\n",
    "                                 ccp_alpha=vector[6],\n",
    "                                 class_weight=balance)\n",
    "\n",
    "    #Feature Selection\n",
    "    X_train,X_test=feature_selection(X_train,\n",
    "                                     X_test,\n",
    "                                     vector[(len(vector)-X_train.shape[1]):])\n",
    "\n",
    "    #Normalization\n",
    "    X_train,X_test=Norm(X_train,X_test,vector[7])\n",
    "\n",
    "    #learning\n",
    "    clf.fit(X_train, y_train)\n",
    "    preds = clf.predict(X_test)\n",
    "\n",
    "    g_score = G_measure(y_test,preds)\n",
    "    lir_score = LIR(X_true,y_test,preds)\n",
    "    fir_score = FIR(y_test,preds)\n",
    "    arr= confusion_matrix(y_test,preds)\n",
    "    pd = arr[1][1]/(arr[1][1]+arr[1][0])\n",
    "    pf = arr[0][1]/(arr[0][0]+arr[0][1])\n",
    "    return g_score, lir_score, fir_score, pd, pf"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 24,
   "metadata": {},
   "outputs": [],
   "source": [
    "from pyharmonysearch import ObjectiveFunctionInterface, harmony_search\n",
    "from math import pow\n",
    "import random\n",
    "from multiprocessing import cpu_count\n",
    "\n",
    "\"\"\"\n",
    " 0 : Criterion                |      {\"gini\",\"entropy\"}\n",
    " 1 : Splitter                 |      {\"best\",\"random\"}\n",
    " 2 : Max Depth                |      [2, 40]\n",
    " 3 : Min Samples Split        |      [2. 100]\n",
    " 4 : Min Samples Leaf         |      [1. 200]\n",
    " 5 : Min Weight Fraction Leaf |      [0.001, 0.1]\n",
    " 6 : CCP Alpha                |      [0.00001, 0.004] \n",
    " 7 : Normalization            |      {\"None\", \"Min-Max\", \"z-score\",\"z-score(source)\", \"z-score(target)\",\"logarithmic filtering\"}\n",
    " 8 : Class Weight             |      [0.01, 1]\n",
    " 9 : Feature selection        |      {\"None\",\"Select\"}\n",
    "\"\"\"\n",
    "\n",
    "class ObjectiveFunction(ObjectiveFunctionInterface):\n",
    "    def __init__(self,X_train,X_test,y_train,y_test):\n",
    "        #define data\n",
    "        self.X_train=X_train\n",
    "        self.X_test=X_test\n",
    "        self.y_train=y_train\n",
    "        self.y_test=y_test\n",
    "        self.lir=[]\n",
    "        self.fir=[]\n",
    "        self.g_measure=[]\n",
    "        self.pd=[]\n",
    "        self.pf=[]\n",
    "        \n",
    "        #define parameters range \n",
    "        self._lower_bounds = [None,None,None,None,None,0.001,0.00001,None,0.01]   # criterion / Max Leaf Nodes / Max Depth / Min Samples Split / Random State / Class Weight\n",
    "        self._upper_bounds = [None,None,None,None,None,0.1,0.004,None,1]\n",
    "        self._discrete_values = [[x for x in range(0, 2)],\n",
    "                                 [x for x in range(0, 2)],\n",
    "                                 [x for x in range(2, 41)],\n",
    "                                 [x for x in range(2, 100)],\n",
    "                                 [x for x in range(1, 200)],\n",
    "                                 None,\n",
    "                                 None,                                \n",
    "                                 [x for x in range(0, 6)],\n",
    "                                 None]\n",
    "        \n",
    "        for i in range(X_train.shape[1]): # Feature Weight\n",
    "            self._lower_bounds.append(None)\n",
    "            self._upper_bounds.append(None)\n",
    "            self._discrete_values.append([x for x in range(0, 2)])\n",
    "            \n",
    "        self._variable = []\n",
    "        for i in range(len(self._lower_bounds)):\n",
    "            self._variable.append(True)\n",
    "\n",
    "        # define all input parameters\n",
    "        self._maximize = True  # do we maximize or minimize?\n",
    "        self._max_imp = 1000  # maximum number of improvisations 1000\n",
    "        self._hms = 100  # harmony memory size 100\n",
    "        self._hmcr = 0.8  # harmony memory considering rate 0.8\n",
    "        self._par = 0.4  # pitch adjusting rate 0.4\n",
    "        self._mpap = 0.25  # maximum pitch adjustment proportion (new parameter defined in pitch_adjustment()) - used for continuous variables only\n",
    "        self._mpai = 10  # maximum pitch adjustment index (also defined in pitch_adjustment()) - used for discrete variables only\n",
    "        self._random_seed = 8675309  # optional random seed for reproducible results\n",
    "    \n",
    "    def get_fitness(self, vector):\n",
    "        # note tune\n",
    "        \n",
    "        #Vector 0 : Criterion\n",
    "        if vector[0]==0:\n",
    "            criterion = \"gini\"\n",
    "        else:\n",
    "            criterion = \"entropy\"\n",
    "        \n",
    "        #Vector 0 : Criterion\n",
    "        if vector[1]==0:\n",
    "            split = \"best\"\n",
    "        else:\n",
    "            split = \"random\"\n",
    "        \n",
    "        #Vectro 6: Class Weight\n",
    "        balance = balance = {True:1,False:vector[8]}\n",
    "\n",
    "        #Model Construct\n",
    "        clf = DecisionTreeClassifier(criterion=criterion,\n",
    "                                     splitter=split,\n",
    "                                     max_depth=vector[2],\n",
    "                                     min_samples_split=vector[3],\n",
    "                                     min_samples_leaf=vector[4],\n",
    "                                     min_weight_fraction_leaf=vector[5],\n",
    "                                     ccp_alpha=vector[6],\n",
    "                                     class_weight=balance)\n",
    "        g=[]\n",
    "        for tr_index, vali_index in kfold.split(self.X_train,self.y_train):\n",
    "            X_tr, X_vali = self.X_train[tr_index], self.X_train[vali_index]\n",
    "            y_tr, y_vali = self.y_train[tr_index], self.y_train[vali_index]\n",
    "            #Feature Selection\n",
    "            X_tr,X_vali=feature_selection(X_tr,X_vali,vector[(len(self._variable)-self.X_train.shape[1]):])\n",
    "\n",
    "            #Normalization\n",
    "            X_tr,X_vali=Norm(X_tr,X_vali,vector[7])\n",
    "\n",
    "            #learning\n",
    "            clf.fit(X_tr, y_tr)\n",
    "            preds = clf.predict(X_vali)\n",
    "            score = G_measure(y_vali,preds)\n",
    "            g.append(score)\n",
    "        g_score=(sum(g)/len(g))\n",
    "        return g_score\n",
    "\n",
    "    def get_value(self, i, j=None):\n",
    "        if self.is_discrete(i):\n",
    "            if j:\n",
    "                return self._discrete_values[i][j]\n",
    "            return self._discrete_values[i][random.randint(0, len(self._discrete_values[i]) - 1)]\n",
    "        if i==6:\n",
    "            return round(random.uniform(self._lower_bounds[i], self._upper_bounds[i]),4)\n",
    "        return round(random.uniform(self._lower_bounds[i], self._upper_bounds[i]),2)\n",
    "\n",
    "    def get_lower_bound(self, i):\n",
    "        return self._lower_bounds[i]\n",
    "\n",
    "    def get_upper_bound(self, i):\n",
    "        return self._upper_bounds[i]\n",
    "    \n",
    "    def get_num_discrete_values(self, i):\n",
    "        if self.is_discrete(i):\n",
    "            return len(self._discrete_values[i])\n",
    "        return float('+inf')\n",
    "\n",
    "    def get_index(self, i, v):\n",
    "        \"\"\"\n",
    "            Because self.discrete_values is in sorted order, we can use binary search.\n",
    "        \"\"\"\n",
    "        return ObjectiveFunction.binary_search(self._discrete_values[i], v)\n",
    "\n",
    "    @staticmethod\n",
    "    def binary_search(a, x):\n",
    "        \"\"\"\n",
    "            Code courtesy Python bisect module: http://docs.python.org/2/library/bisect.html#searching-sorted-lists\n",
    "        \"\"\"\n",
    "        i = bisect_left(a, x)\n",
    "        if i != len(a) and a[i] == x:\n",
    "            return i\n",
    "        raise ValueError\n",
    "\n",
    "    def is_variable(self, i):\n",
    "        return self._variable[i]\n",
    "\n",
    "    def is_discrete(self, i):\n",
    "        return self._discrete_values[i] is not None\n",
    "\n",
    "    def get_num_parameters(self):\n",
    "        return len(self._lower_bounds)\n",
    "\n",
    "    def use_random_seed(self):\n",
    "        return hasattr(self, '_random_seed') and self._random_seed\n",
    "\n",
    "    def get_random_seed(self):\n",
    "        return self._random_seed\n",
    "\n",
    "    def get_max_imp(self):\n",
    "        return self._max_imp\n",
    "\n",
    "    def get_hmcr(self):\n",
    "        return self._hmcr\n",
    "\n",
    "    def get_par(self):\n",
    "        return self._par\n",
    "\n",
    "    def get_hms(self):\n",
    "        return self._hms\n",
    "\n",
    "    def get_mpai(self):\n",
    "        return self._mpai\n",
    "\n",
    "    def get_mpap(self):\n",
    "        return self._mpap\n",
    "\n",
    "    def maximize(self):\n",
    "        return self._maximize\n",
    "    \n",
    "    def get_lir(self):\n",
    "        index=self.get_bestindex()\n",
    "        return self.lir[index]\n",
    "    \n",
    "    def get_fir(self):\n",
    "        print(\"b\")\n",
    "#         index=self.get_bestindex()\n",
    "#         return self.fir[index]\n",
    "    \n",
    "    def get_pd_pf(self,y_true,y_pred):\n",
    "        arr = confusion_matrix(y_true,y_pred)\n",
    "        pd = arr[1][1]/(arr[1][1]+arr[1][0])\n",
    "        pf = arr[0][1]/(arr[0][0]+arr[0][1])\n",
    "        return pd, pf \n",
    "    \n",
    "    def get_bestindex(self):\n",
    "        return self.g_measure.index(max(self.g_measure))\n",
    "        "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 25,
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "df = pandas.read_csv('../Dataset/AEEEM/EQ.csv')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 26,
   "metadata": {},
   "outputs": [],
   "source": [
    "X = np.array(df.loc[:,'ck_oo_numberOfPrivateMethods':'LDHH_numberOfMethods'])\n",
    "y = np.array(df['class'])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 27,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/home/tako/anaconda3/envs/jaewook_p37_2/lib/python3.7/site-packages/sklearn/model_selection/_split.py:297: FutureWarning: Setting a random_state has no effect since shuffle is False. This will raise an error in 0.24. You should leave random_state to its default (None), or set shuffle=True.\n",
      "  FutureWarning\n"
     ]
    }
   ],
   "source": [
    "from sklearn.model_selection import StratifiedKFold\n",
    "num_split=10\n",
    "kf = StratifiedKFold(n_splits=num_split,random_state=1,shuffle=False)\n",
    "kfold = StratifiedKFold(n_splits=(num_split-1),shuffle=False)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 28,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "5b599551b89c4d25a47fa6bb3195d39e",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "HBox(children=(FloatProgress(value=1.0, bar_style='info', layout=Layout(width='20px'), max=1.0), HTML(value=''â€¦"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "-------------------------------------------------------------------------------------------------\n",
      "Elapsed time: 0:01:37.563002\n",
      "Best harmony: [0, 0, 3, 99, 12, 0.02, 0.0029]\n",
      "Normalization: 5\n",
      "Class Weight: 0.5692229867474252\n",
      "Best fitness: 0.7836762838306578\n",
      "[0, 0, 3, 99, 12, 0.02, 0.0029, 5, 0.5692229867474252]\n",
      "G measure: 0.8275862068965517\n",
      "LIR: 0.08253707797161719\n",
      "FIR: 0.441919191919192\n",
      "PD: 0.9230769230769231\n",
      "PF: 0.25\n",
      "-------------------------------------------------------------------------------------------------\n",
      "Elapsed time: 0:01:39.765502\n",
      "Best harmony: [0, 0, 4, 13, 3, 0.03, 0.0030968856302910595]\n",
      "Normalization: 4\n",
      "Class Weight: 0.64\n",
      "Best fitness: 0.7980812984643497\n",
      "[0, 0, 4, 13, 3, 0.03, 0.0030968856302910595, 4, 0.64]\n",
      "G measure: 0.641399416909621\n",
      "LIR: 0.14658483504440523\n",
      "FIR: 0.2515151515151515\n",
      "PD: 0.7692307692307693\n",
      "PF: 0.45\n",
      "-------------------------------------------------------------------------------------------------\n",
      "Elapsed time: 0:01:38.971223\n",
      "Best harmony: [1, 0, 22, 18, 16, 0.02, 0.0006298195727087557]\n",
      "Normalization: 1\n",
      "Class Weight: 0.89\n",
      "Best fitness: 0.7791220078634252\n",
      "[1, 0, 22, 18, 16, 0.02, 0.0006298195727087557, 1, 0.89]\n",
      "G measure: 0.746928746928747\n",
      "LIR: 0.15516320311049844\n",
      "FIR: 0.5568181818181819\n",
      "PD: 0.6153846153846154\n",
      "PF: 0.05\n",
      "-------------------------------------------------------------------------------------------------\n",
      "Elapsed time: 0:01:37.263474\n",
      "Best harmony: [1, 1, 34, 23, 96, 0.08, 0.0016071967372830083]\n",
      "Normalization: 5\n",
      "Class Weight: 0.38\n",
      "Best fitness: 0.7739016522155979\n",
      "[1, 1, 34, 23, 96, 0.08, 0.0016071967372830083, 5, 0.38]\n",
      "G measure: 0.5397923875432526\n",
      "LIR: -0.08496481040622622\n",
      "FIR: 0.14646464646464655\n",
      "PD: 0.46153846153846156\n",
      "PF: 0.35\n",
      "-------------------------------------------------------------------------------------------------\n",
      "Elapsed time: 0:01:39.766211\n",
      "Best harmony: [1, 0, 15, 96, 1, 0.04720004117233943, 0.0006]\n",
      "Normalization: 0\n",
      "Class Weight: 0.5\n",
      "Best fitness: 0.781924164046678\n",
      "[1, 0, 15, 96, 1, 0.04720004117233943, 0.0006, 0, 0.5]\n",
      "G measure: 0.8125000000000001\n",
      "LIR: 0.09127725856697821\n",
      "FIR: 0.40625\n",
      "PD: 1.0\n",
      "PF: 0.3157894736842105\n",
      "-------------------------------------------------------------------------------------------------\n",
      "Elapsed time: 0:01:40.967451\n",
      "Best harmony: [1, 0, 26, 37, 2, 0.09808764007847622, 0.002]\n",
      "Normalization: 5\n",
      "Class Weight: 0.7550191786082091\n",
      "Best fitness: 0.7816053265216473\n",
      "[1, 0, 26, 37, 2, 0.09808764007847622, 0.002, 5, 0.7550191786082091]\n",
      "G measure: 0.672316384180791\n",
      "LIR: 0.1326755644708143\n",
      "FIR: 0.4776785714285714\n",
      "PD: 0.5384615384615384\n",
      "PF: 0.10526315789473684\n",
      "-------------------------------------------------------------------------------------------------\n",
      "Elapsed time: 0:01:40.868658\n",
      "Best harmony: [1, 0, 11, 93, 17, 0.05, 0.0019765448497739663]\n",
      "Normalization: 5\n",
      "Class Weight: 0.349315640013863\n",
      "Best fitness: 0.8025719260684394\n",
      "[1, 0, 11, 93, 17, 0.05, 0.0019765448497739663, 5, 0.349315640013863]\n",
      "G measure: 0.7138810198300284\n",
      "LIR: 0.1982993770941555\n",
      "FIR: 0.3680555555555555\n",
      "PD: 0.6923076923076923\n",
      "PF: 0.2631578947368421\n",
      "-------------------------------------------------------------------------------------------------\n",
      "Elapsed time: 0:01:39.468363\n",
      "Best harmony: [0, 0, 27, 91, 14, 0.01689825200356248, 0.0010714323517025384]\n",
      "Normalization: 3\n",
      "Class Weight: 0.5059061014407298\n",
      "Best fitness: 0.7769659110245706\n",
      "[0, 0, 27, 91, 14, 0.01689825200356248, 0.0010714323517025384, 3, 0.5059061014407298]\n",
      "G measure: 0.6875\n",
      "LIR: 0.040325600337743586\n",
      "FIR: 0.29829545454545453\n",
      "PD: 0.8461538461538461\n",
      "PF: 0.42105263157894735\n",
      "-------------------------------------------------------------------------------------------------\n",
      "Elapsed time: 0:01:39.766903\n",
      "Best harmony: [1, 0, 32, 3, 37, 0.1, 0.0025617694408855726]\n",
      "Normalization: 1\n",
      "Class Weight: 0.667932036045299\n",
      "Best fitness: 0.7868578939331102\n",
      "[1, 0, 32, 3, 37, 0.1, 0.0025617694408855726, 1, 0.667932036045299]\n",
      "G measure: 0.7792207792207793\n",
      "LIR: 0.1680094332954978\n",
      "FIR: 0.43125\n",
      "PD: 0.7692307692307693\n",
      "PF: 0.21052631578947367\n",
      "-------------------------------------------------------------------------------------------------\n",
      "Elapsed time: 0:01:38.665888\n",
      "Best harmony: [0, 0, 36, 91, 12, 0.018361466730216117, 0.002443579639198268]\n",
      "Normalization: 4\n",
      "Class Weight: 0.3687556324149712\n",
      "Best fitness: 0.7668504996374763\n",
      "[0, 0, 36, 91, 12, 0.018361466730216117, 0.002443579639198268, 4, 0.3687556324149712]\n",
      "G measure: 0.8235294117647058\n",
      "LIR: 0.033639143730886834\n",
      "FIR: 0.4375\n",
      "PD: 1.0\n",
      "PF: 0.3\n",
      "\n",
      "G_measure : 0.7831556963605951\n"
     ]
    }
   ],
   "source": [
    "g_measure=[]\n",
    "best_harmony=[]\n",
    "col = [\"Criterion\",\"Splitter\",\"Max Depth\",\"Min Samples Split\",\"Min Samples Leaf\",\"Min Weight Fraction Leaf\",\"CCP Alpha\",\"Normalization\",\n",
    "      \"Class Weight(False)\"]\n",
    "for i in range(X.shape[1]):\n",
    "    col.append(i+1)\n",
    "for train_index, test_index in notebook.tqdm(kf.split(X,y)):\n",
    "    print('-------------------------------------------------------------------------------------------------')\n",
    "    X_train, X_test = X[train_index], X[test_index]\n",
    "    y_train, y_test = y[train_index], y[test_index]\n",
    "    obj_fun = ObjectiveFunction(X_train=X_train,X_test=X_test,y_train=y_train,y_test=y_test)\n",
    "    num_processes = 1  # use number of logical CPUs\n",
    "    num_iterations = 5 # each process does 5 iterations\n",
    "    results = harmony_search(obj_fun, num_processes, num_iterations)\n",
    "    index = (len(results.best_harmony)-X_train.shape[1])\n",
    "    print('Elapsed time: {}\\nBest harmony: {}\\nNormalization: {}\\nClass Weight: {}\\nBest fitness: {}'.format(results.elapsed_time, results.best_harmony[:7], results.best_harmony[7],results.best_harmony[8],results.best_fitness))\n",
    "    g_measure.append(results.best_fitness)\n",
    "    best_harmony.append(results.best_harmony)\n",
    "    g,lir,fir,pd,pf = classifier(X_train,X_test,y_train,y_test,results.best_harmony)\n",
    "    print('G measure: {}\\nLIR: {}\\nFIR: {}\\nPD: {}\\nPF: {}'.format(g,lir,fir,pd,pf))\n",
    "\n",
    "print(\"G_measure : {}\".format(sum(g_measure)/len(g_measure)))\n",
    "# find best harmony"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 29,
   "metadata": {},
   "outputs": [],
   "source": [
    "# df_best_h= pandas.DataFrame(best_harmony,columns=col)\n",
    "# df_best_g= pandas.DataFrame(g_measure,columns=[\"G Measure\"])\n",
    "# df_best_harmony = pandas.concat([df_best_h,df_best_g],axis=1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 30,
   "metadata": {},
   "outputs": [],
   "source": [
    "# import os\n",
    "# if not os.path.exists('Best Harmony_AEEEM.csv'):\n",
    "#     df_best_harmony.to_csv('Best Harmony_AEEEM.csv', index=False, mode='w', encoding='utf-8-sig')\n",
    "# else:\n",
    "#     df_best_harmony.to_csv('Best Harmony_AEEEM.csv', index=False, mode='a', encoding='utf-8-sig', header=False)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.7"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
